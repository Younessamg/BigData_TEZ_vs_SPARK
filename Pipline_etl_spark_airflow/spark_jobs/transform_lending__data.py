import sys
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, when, regexp_extract, to_date, year,
    round as spark_round, trim, upper, avg, min as spark_min, max as spark_max
)
from pyspark.sql.types import DoubleType, IntegerType
from spark_config import SELECTED_COLUMNS, DEFAULT_STATUSES, SPARK_CONFIG


def create_spark_session():
    builder = SparkSession.builder.appName("LendingClubETL")
    for key, value in SPARK_CONFIG.items():
        builder = builder.config(key, value)
    return builder.getOrCreate()


def extract_data(spark, input_path):
    """Extrait les donnÃ©es depuis le fichier CSV source."""
    start_time = time.time()
    print("=" * 80)
    print("ğŸ“‚ Ã‰TAPE 1 : EXTRACTION DES DONNÃ‰ES")
    print("=" * 80)
    print(f"ğŸ“ Fichier source : {input_path}")
    print("â³ Lecture du fichier CSV...")
    
    extract_start = time.time()
    df = spark.read.csv(input_path, header=True, inferSchema=True, multiLine=True, escape='"')
    extract_time = time.time() - extract_start
    
    print(f"âœ… Fichier chargÃ© en {extract_time:.2f} secondes")
    
    # Compter les lignes
    print("â³ Comptage des lignes...")
    count_start = time.time()
    row_count = df.count()
    count_time = time.time() - count_start
    
    total_time = time.time() - start_time
    
    print(f"ğŸ“Š Lignes extraites : {row_count:,}")
    print(f"â±ï¸  Temps d'exÃ©cution : {total_time:.2f} secondes")
    print(f"   â”œâ”€ Lecture CSV : {extract_time:.2f}s")
    print(f"   â””â”€ Comptage : {count_time:.2f}s")
    print("=" * 80)
    
    return df


def transform_data(df):
    """Applique toutes les transformations de donnÃ©es."""
    total_start = time.time()
    print("\n" + "=" * 80)
    print("ğŸ”„ Ã‰TAPE 2 : TRANSFORMATION DES DONNÃ‰ES")
    print("=" * 80)
    
    initial_count = df.count()
    print(f"ğŸ“Š Lignes initiales : {initial_count:,}")
    
    # 1. SÃ‰LECTION DES COLONNES
    step_start = time.time()
    print(f"\nğŸ“‹ [2.1] SÃ©lection des colonnes ({len(SELECTED_COLUMNS)} colonnes)...")
    df = df.select(*SELECTED_COLUMNS)
    step_time = time.time() - step_start
    print(f"   âœ… Colonnes sÃ©lectionnÃ©es : {len(SELECTED_COLUMNS)}")
    print(f"   â±ï¸  Temps d'exÃ©cution : {step_time:.2f} secondes")
    
    # 2. FILTRAGE DES NULLS
    step_start = time.time()
    print(f"\nğŸ” [2.2] Filtrage des valeurs nulles...")
    print(f"   â³ Suppression des lignes avec loan_status, annual_inc ou issue_d null...")
    before_filter = df.count()
    df = df.filter(col('loan_status').isNotNull() & 
                   col('annual_inc').isNotNull() & 
                   col('issue_d').isNotNull())
    after_filter = df.count()
    step_time = time.time() - step_start
    removed = before_filter - after_filter
    print(f"   âœ… Lignes supprimÃ©es : {removed:,} ({removed/before_filter*100:.2f}%)")
    print(f"   ğŸ“Š Lignes restantes : {after_filter:,}")
    print(f"   â±ï¸  Temps d'exÃ©cution : {step_time:.2f} secondes")
    
    # 3. SUPPRESSION DES DOUBLONS
    step_start = time.time()
    print(f"\nğŸ—‘ï¸  [2.3] Suppression des doublons (sur colonne 'id')...")
    before_dedup = df.count()
    df = df.dropDuplicates(['id'])
    after_dedup = df.count()
    step_time = time.time() - step_start
    duplicates = before_dedup - after_dedup
    print(f"   âœ… Doublons supprimÃ©s : {duplicates:,}")
    print(f"   ğŸ“Š Lignes uniques : {after_dedup:,}")
    print(f"   â±ï¸  Temps d'exÃ©cution : {step_time:.2f} secondes")
    
    # 4. NORMALISATION DES DONNÃ‰ES
    step_start = time.time()
    print(f"\nğŸ”§ [2.4] Normalisation des colonnes...")
    
    # Normalisation term
    norm_start = time.time()
    df = df.withColumn('term', regexp_extract(col('term'), r'(\d+)', 1).cast(IntegerType()))
    print(f"   âœ… 'term' normalisÃ© (regexp + cast Integer) : {time.time() - norm_start:.2f}s")
    
    # Normalisation int_rate
    norm_start = time.time()
    df = df.withColumn('int_rate', regexp_extract(col('int_rate'), r'([\d.]+)', 1).cast(DoubleType()))
    print(f"   âœ… 'int_rate' normalisÃ© (regexp + cast Double) : {time.time() - norm_start:.2f}s")
    
    # Normalisation revol_util
    norm_start = time.time()
    df = df.withColumn('revol_util', regexp_extract(col('revol_util'), r'([\d.]+)', 1).cast(DoubleType()))
    print(f"   âœ… 'revol_util' normalisÃ© (regexp + cast Double) : {time.time() - norm_start:.2f}s")
    
    # Normalisation emp_length
    norm_start = time.time()
    df = df.withColumn('emp_length',
        when(col('emp_length').contains('10+'), 10)
        .when(col('emp_length').contains('< 1'), 0)
        .otherwise(regexp_extract(col('emp_length'), r'(\d+)', 1))
        .cast(IntegerType()))
    print(f"   âœ… 'emp_length' normalisÃ© (conditions + regexp) : {time.time() - norm_start:.2f}s")
    
    step_time = time.time() - step_start
    print(f"   â±ï¸  Temps total normalisation : {step_time:.2f} secondes")
    
    # 5. CONVERSION DES DATES
    step_start = time.time()
    print(f"\nğŸ“… [2.5] Conversion des dates...")
    df = df.withColumn('issue_d', to_date(col('issue_d'), 'MMM-yyyy'))
    step_time = time.time() - step_start
    print(f"   âœ… Colonne 'issue_d' convertie (format: MMM-yyyy)")
    print(f"   â±ï¸  Temps d'exÃ©cution : {step_time:.2f} secondes")
    
    # 6. CRÃ‰ATION DE LA VARIABLE CIBLE
    step_start = time.time()
    print(f"\nğŸ¯ [2.6] CrÃ©ation de la variable cible 'is_default'...")
    print(f"   ğŸ“‹ Status considÃ©rÃ©s comme dÃ©faut : {DEFAULT_STATUSES}")
    df = df.withColumn('is_default',
        when(col('loan_status').isin(DEFAULT_STATUSES), 1).otherwise(0))
    step_time = time.time() - step_start
    print(f"   âœ… Variable cible crÃ©Ã©e (0 = Non-dÃ©faut, 1 = DÃ©faut)")
    print(f"   â±ï¸  Temps d'exÃ©cution : {step_time:.2f} secondes")
    
    # 7. FEATURE ENGINEERING
    step_start = time.time()
    print(f"\nâœ¨ [2.7] Feature Engineering...")
    
    # Feature : fico_avg
    feat_start = time.time()
    df = df.withColumn('fico_avg',
        spark_round((col('fico_range_low') + col('fico_range_high')) / 2, 0).cast(IntegerType()))
    print(f"   âœ… 'fico_avg' crÃ©Ã© (moyenne de fico_range_low/high) : {time.time() - feat_start:.2f}s")
    
    # Feature : income_to_loan_ratio
    feat_start = time.time()
    df = df.withColumn('income_to_loan_ratio',
        when(col('loan_amnt') > 0, spark_round(col('annual_inc') / col('loan_amnt'), 2))
        .otherwise(None).cast(DoubleType()))
    print(f"   âœ… 'income_to_loan_ratio' crÃ©Ã© (annual_inc / loan_amnt) : {time.time() - feat_start:.2f}s")
    
    # Feature : year
    feat_start = time.time()
    df = df.withColumn('year', year(col('issue_d')))
    print(f"   âœ… 'year' crÃ©Ã© (extrait de issue_d) : {time.time() - feat_start:.2f}s")
    
    step_time = time.time() - step_start
    print(f"   â±ï¸  Temps total feature engineering : {step_time:.2f} secondes")
    
    # 8. NORMALISATION DES STRINGS
    step_start = time.time()
    print(f"\nğŸ”¤ [2.8] Normalisation des colonnes textuelles...")
    text_cols = ['grade', 'sub_grade', 'home_ownership', 'verification_status', 
                 'loan_status', 'purpose', 'addr_state']
    print(f"   ğŸ“‹ Colonnes Ã  normaliser : {len(text_cols)} colonnes")
    for i, col_name in enumerate(text_cols, 1):
        norm_start = time.time()
        df = df.withColumn(col_name, upper(trim(col(col_name))))
        print(f"   âœ… [{i}/{len(text_cols)}] '{col_name}' normalisÃ© (UPPER + TRIM) : {time.time() - norm_start:.2f}s")
    step_time = time.time() - step_start
    print(f"   â±ï¸  Temps total normalisation textuelle : {step_time:.2f} secondes")
    
    # RÃ©capitulatif final
    final_count = df.count()
    total_time = time.time() - total_start
    
    print("\n" + "=" * 80)
    print("âœ… TRANSFORMATION TERMINÃ‰E")
    print("=" * 80)
    print(f"ğŸ“Š Lignes initiales : {initial_count:,}")
    print(f"ğŸ“Š Lignes finales : {final_count:,}")
    print(f"ğŸ“‰ Lignes supprimÃ©es : {initial_count - final_count:,} ({(initial_count - final_count)/initial_count*100:.2f}%)")
    print(f"â±ï¸  Temps total de transformation : {total_time:.2f} secondes ({total_time/60:.2f} minutes)")
    print(f"âš¡ Performance : {final_count/total_time:,.0f} lignes/seconde")
    print("=" * 80)
    
    return df


def load_data(df, output_path):
    """Charge les donnÃ©es transformÃ©es dans le fichier de sortie."""
    start_time = time.time()
    print("\n" + "=" * 80)
    print("ğŸ’¾ Ã‰TAPE 3 : CHARGEMENT DES DONNÃ‰ES")
    print("=" * 80)
    print(f"ğŸ“ Dossier de sortie : {output_path}")
    
    # Compter les lignes avant Ã©criture
    print("â³ Comptage des lignes Ã  Ã©crire...")
    count_start = time.time()
    row_count = df.count()
    count_time = time.time() - count_start
    print(f"ğŸ“Š Lignes Ã  Ã©crire : {row_count:,} ({count_time:.2f}s)")
    
    # PrÃ©parer le DataFrame pour Ã©criture (un seul fichier)
    print("â³ PrÃ©paration du DataFrame (coalesce Ã  1 partition)...")
    prep_start = time.time()
    df_output = df.coalesce(1)
    prep_time = time.time() - prep_start
    print(f"âœ… DataFrame prÃ©parÃ© ({prep_time:.2f}s)")
    
    # Ã‰criture
    print("â³ Ã‰criture du fichier CSV...")
    write_start = time.time()
    df_output.write.mode('overwrite').option('header', 'true').csv(output_path)
    write_time = time.time() - write_start
    
    total_time = time.time() - start_time
    
    print("=" * 80)
    print("âœ… CHARGEMENT TERMINÃ‰")
    print("=" * 80)
    print(f"ğŸ“Š Lignes Ã©crites : {row_count:,}")
    print(f"â±ï¸  Temps d'exÃ©cution : {total_time:.2f} secondes ({total_time/60:.2f} minutes)")
    print(f"   â”œâ”€ Comptage : {count_time:.2f}s")
    print(f"   â”œâ”€ PrÃ©paration : {prep_time:.2f}s")
    print(f"   â””â”€ Ã‰criture CSV : {write_time:.2f}s")
    print(f"âš¡ DÃ©bit d'Ã©criture : {row_count/write_time:,.0f} lignes/seconde")
    print("=" * 80)


def main(input_path, output_path):
    import sys
    
    pipeline_start = time.time()
    print("ğŸš€ PIPELINE ETL LENDING CLUB")
    print("=" * 80)
    print(f"ğŸ• DÃ©marrage : {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    spark_start = time.time()
    spark = create_spark_session()
    spark_init_time = time.time() - spark_start
    print(f"âš™ï¸  Spark Session initialisÃ©e : {spark_init_time:.2f}s")
    
    try:
        df_raw = extract_data(spark, input_path)
        df_clean = transform_data(df_raw)
        
        # Cache le DataFrame pour Ã©viter de recalculer
        df_clean = df_clean.cache()
        
        # Calculer le total une seule fois
        total_rows = df_clean.count()
        print(f"\nğŸ“Š Total de lignes aprÃ¨s transformation : {total_rows:,}")
        
        # AperÃ§u des donnÃ©es
        print("\n" + "=" * 80)
        print("ğŸ“Š APERÃ‡U DES DONNÃ‰ES (5 premiÃ¨res lignes)")
        print("=" * 80)
        sys.stdout.flush()  # Force l'affichage immÃ©diat
        df_clean.select('loan_amnt', 'int_rate', 'grade', 'is_default', 'fico_avg', 'year').show(5, truncate=False)
        sys.stdout.flush()
        
        # Statistiques dÃ©taillÃ©es - Distribution is_default
        print("\n" + "=" * 80)
        print("ğŸ“ˆ DISTRIBUTION DE LA VARIABLE CIBLE (is_default)")
        print("=" * 80)
        sys.stdout.flush()
        default_dist = df_clean.groupBy('is_default').count().orderBy('is_default')
        default_dist.show(truncate=False)
        sys.stdout.flush()
        
        # Calculer le pourcentage avec formatage clair
        default_stats = default_dist.collect()
        print("\nğŸ“Š RÃ‰SUMÃ‰ is_default :")
        print("-" * 80)
        for row in default_stats:
            count = row['count']
            pct = (count / total_rows) * 100
            status = "DÃ‰FAUT (1)" if row['is_default'] == 1 else "NON-DÃ‰FAUT (0)"
            print(f"  â€¢ {status:20s}: {count:>12,} lignes ({pct:>6.2f}%)")
        print("-" * 80)
        sys.stdout.flush()
        
        # Statistiques par annÃ©e
        print("\n" + "=" * 80)
        print("ğŸ“… DISTRIBUTION PAR ANNÃ‰E")
        print("=" * 80)
        sys.stdout.flush()
        year_dist = df_clean.groupBy('year').count().orderBy('year')
        year_dist.show(truncate=False)
        sys.stdout.flush()
        
        # Statistiques par grade
        print("\n" + "=" * 80)
        print("â­ DISTRIBUTION PAR GRADE")
        print("=" * 80)
        sys.stdout.flush()
        grade_dist = df_clean.groupBy('grade').count().orderBy('grade')
        grade_dist.show(truncate=False)
        sys.stdout.flush()
        
        # Statistiques numÃ©riques
        print("\n" + "=" * 80)
        print("ğŸ”¢ STATISTIQUES NUMÃ‰RIQUES (min, max, moyenne, Ã©cart-type)")
        print("=" * 80)
        sys.stdout.flush()
        df_clean.select(
            'loan_amnt', 'int_rate', 'annual_inc', 'fico_avg', 
            'income_to_loan_ratio', 'dti'
        ).describe().show(truncate=False)
        sys.stdout.flush()
        
        # RÃ©sumÃ© des mÃ©triques clÃ©s
        print("\n" + "=" * 80)
        print("ğŸ“‹ RÃ‰SUMÃ‰ DES MÃ‰TRIQUES")
        print("=" * 80)
        print(f"  â€¢ Total lignes traitÃ©es      : {total_rows:>12,}")
        print(f"  â€¢ Nombre de colonnes         : {len(df_clean.columns):>12}")
        
        # Calculer quelques statistiques supplÃ©mentaires
        loan_stats = df_clean.select(
            spark_min('loan_amnt').alias('min_loan'),
            spark_max('loan_amnt').alias('max_loan'),
            avg('loan_amnt').alias('avg_loan'),
            avg('int_rate').alias('avg_int_rate'),
            avg('fico_avg').alias('avg_fico')
        ).collect()[0]
        
        print(f"  â€¢ Montant prÃªt (min)          : ${loan_stats['min_loan']:>11,.2f}")
        print(f"  â€¢ Montant prÃªt (max)          : ${loan_stats['max_loan']:>11,.2f}")
        print(f"  â€¢ Montant prÃªt (moyenne)      : ${loan_stats['avg_loan']:>11,.2f}")
        print(f"  â€¢ Taux intÃ©rÃªt (moyenne)      : {loan_stats['avg_int_rate']:>11.2f}%")
        print(f"  â€¢ Score FICO (moyenne)        : {loan_stats['avg_fico']:>11.2f}")
        
        # Compter les annÃ©es uniques
        year_list = [row['year'] for row in df_clean.select('year').distinct().collect()]
        print(f"  â€¢ AnnÃ©es couvertes            : {min(year_list)} - {max(year_list)} ({len(year_list)} annÃ©es)")
        
        print("=" * 80)
        sys.stdout.flush()
        
        load_data(df_clean, output_path)
        
        # RÃ©sumÃ© global du pipeline
        pipeline_time = time.time() - pipeline_start
        print("\n" + "=" * 80)
        print("ğŸ‰ PIPELINE TERMINÃ‰ AVEC SUCCÃˆS!")
        print("=" * 80)
        print(f"ğŸ“ Fichier de sortie : {output_path}")
        print(f"ğŸ“Š Total lignes traitÃ©es : {total_rows:,}")
        print(f"ğŸ• Heure de fin : {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â±ï¸  Temps total d'exÃ©cution : {pipeline_time:.2f} secondes ({pipeline_time/60:.2f} minutes)")
        print("=" * 80)
        sys.stdout.flush()
        
    except Exception as e:
        print("\n" + "=" * 80)
        print(f"âŒ ERREUR : {e}")
        import traceback
        print(traceback.format_exc())
        print("=" * 80)
        raise
    finally:
        spark.stop()


if __name__ == '__main__':
    if len(sys.argv) != 3:
        print("Usage: python transform_lending_data.py  ")
        sys.exit(1)
    main(sys.argv[1], sys.argv[2])